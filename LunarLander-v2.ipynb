{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f389030f-a5c8-4df8-8356-bdb9b23e9969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f66b9cb-0ec3-4f1a-a3ff-a9d2f5cddf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient-utils 0.5.0 requires wheel<0.36.0,>=0.35.1, but you have wheel 0.44.0 which is incompatible.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q wheel setuptools pip swig --upgrade\n",
    "!pip install -q blinker --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d475f14-842e-49e4-826c-fe9bbe17bdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbf7883-2e9a-4753-bbd0-c51c184fa715",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvidia-pyindex\n",
      "  Downloading nvidia-pyindex-1.0.9.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvidia-pyindex\n",
      "  Building wheel for nvidia-pyindex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvidia-pyindex: filename=nvidia_pyindex-1.0.9-py3-none-any.whl size=8419 sha256=110c8d93639fc0b60ca21d0596e1b788694d7b5b48b30077437a6d9cbfebcb1d\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/d0/7d/b68b3665d16ee20355e65fb7ef48b7ca26533217d9f09924fe\n",
      "Successfully built nvidia-pyindex\n",
      "Installing collected packages: nvidia-pyindex\n",
      "Successfully installed nvidia-pyindex-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvidia-tensorrt\n",
      "  Downloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl.metadata (596 bytes)\n",
      "Collecting tensorrt (from nvidia-tensorrt)\n",
      "  Downloading tensorrt-10.4.0.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorrt-cu12==10.4.0 (from tensorrt->nvidia-tensorrt)\n",
      "  Downloading tensorrt-cu12-10.4.0.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading nvidia_tensorrt-99.0.0-py3-none-manylinux_2_17_x86_64.whl (17 kB)\n",
      "Building wheels for collected packages: tensorrt, tensorrt-cu12\n",
      "  Building wheel for tensorrt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt: filename=tensorrt-10.4.0-py2.py3-none-any.whl size=16337 sha256=18beb1c99ecb15556b0bccb1c49aa1b125130482a09718ace846e00cea858bb1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eqrq_x6u/wheels/93/75/7a/07026f58bd417b806c7758fbc0f1bb0d5f2326c0eb37a246fc\n",
      "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.4.0-py2.py3-none-any.whl size=17555 sha256=30d5a03147b73aea4c6380fae2e37bccd1c8a6d7ad0fc684cbf1f11f97e28c2a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eqrq_x6u/wheels/df/4b/55/389463513523d11b72443d23c8ad47c6f925d66af6221cc384\n",
      "Successfully built tensorrt tensorrt-cu12\n",
      "Installing collected packages: tensorrt-cu12, tensorrt, nvidia-tensorrt\n",
      "Successfully installed nvidia-tensorrt-99.0.0 tensorrt-10.4.0 tensorrt-cu12-10.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd8c4aa2-4f45-4e56-b279-0d6138f0cfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q stable-baselines3 \"gymnasium[box2d]\" tensorflow[and-cuda] dagshub mlflow hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2794c57e-6164-4e64-b214-0800d3396f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81bb0e3f-6399-40d3-844e-47c48a377922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorRT version: 10.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "print(\"TensorRT version:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc19beb-5e67-4482-b3bc-5b1ad095e539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5a461e-d0f3-49b6-ad14-c5aaf4588720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Open the following link in your browser to authorize the client:\n",
      "https://dagshub.com/login/oauth/authorize?state=c869c66b-6acc-4ca1-8169-b36be1a9854b&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=87855edcb7374861d92e2dae45e173d19c68bf5a92c0b455109f151286e5af7a\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as smileynet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as smileynet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"smileynet/gymnasium_experiments\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"smileynet/gymnasium_experiments\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository smileynet/gymnasium_experiments initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository smileynet/gymnasium_experiments initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='smileynet', repo_name='gymnasium_experiments', mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe64222-285d-4d08-98c4-a74e0057f3c7",
   "metadata": {},
   "source": [
    "## Determine Best Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6bc7dc5-24c3-4884-9853-a768c033361e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU FPS: 385.38\n",
      "GPU FPS: 435.07\n",
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "TOTAL_TIMESTEPS = 100000  # Short benchmark to measure FPS\n",
    "\n",
    "def measure_fps(device):\n",
    "    \"\"\"\n",
    "    Measure FPS on the given device (cpu or cuda).\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    # Initialize the model on the specified device\n",
    "    model = PPO('MlpPolicy', env, device=device)\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train for a small number of timesteps (benchmark)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    elapsed_time = end_time - start_time\n",
    "    fps = TOTAL_TIMESTEPS / elapsed_time\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n",
    "\n",
    "    return fps\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Measure FPS on CPU\n",
    "cpu_fps = measure_fps(device=\"cpu\")\n",
    "print(f\"CPU FPS: {cpu_fps:.2f}\")\n",
    "\n",
    "# Measure FPS on GPU (if available)\n",
    "if gpu_available:\n",
    "    gpu_fps = measure_fps(device=\"cuda\")\n",
    "    print(f\"GPU FPS: {gpu_fps:.2f}\")\n",
    "else:\n",
    "    gpu_fps = 0\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "# Select the device with the higher FPS\n",
    "if gpu_fps > cpu_fps:\n",
    "    print(\"Using GPU for training.\")\n",
    "    chosen_device = \"cuda\"\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "    chosen_device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dee7f372-0f38-454e-af20-9cd501e5759f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "def ensure_directory_exists(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    return new_dir\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "                    \n",
    "\n",
    "def set_mlflow_tags(hparams, trial_number):\n",
    "    mlflow.set_tag(\"trial_number\", trial_number)\n",
    "    mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "    mlflow.set_tag(\"model_type\", \"PPO\")\n",
    "    mlflow.set_tag(\"policy_type\", \"MlpPolicy\")\n",
    "    mlflow.set_tag(\"environment_name\", \"LunarLander-v2\")\n",
    "    mlflow.set_tag(\"total_timesteps\", TOTAL_TIMESTEPS)\n",
    "    mlflow.set_tag(\"python_version\", sys.version)\n",
    "    mlflow.set_tag(\"stable_baselines3_version\", stable_baselines3.__version__)\n",
    "    mlflow.set_tag(\"device\", DEVICE)\n",
    "    mlflow.log_params(hparams)\n",
    "    \n",
    "def create_model(hparams, env):\n",
    "    return PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=DEVICE,\n",
    "        verbose=2,\n",
    "        **hparams\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, eval_env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = eval_env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward\n",
    "\n",
    "def train_and_evaluate(hparams, trial_number):\n",
    "    global best_mean_reward\n",
    "    global models_dir\n",
    "    global best_model_path\n",
    "    \n",
    "    # Set up environments\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "    eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = create_model(hparams, env)\n",
    "\n",
    "        # Set up logging\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[MLflowOutputFormat()], # HumanOutputFormat(sys.stdout), \n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mean_reward = evaluate_model(model, eval_env)\n",
    "\n",
    "        # If best model, save it\n",
    "        if mean_reward > best_mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "            model_save_path = os.path.join(models_dir, f\"model_trial_{trial_number}.zip\")\n",
    "            model.save(model_save_path)\n",
    "            model.save(best_model_path)\n",
    "            mlflow.log_artifact(model_save_path)\n",
    "        \n",
    "        # Log the evaluation metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Return the loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "    finally:\n",
    "        # Ensure environments are properly closed\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "        \n",
    "def save_trials(trials):\n",
    "    with open(\"trials.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)        \n",
    "\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set MLflow tags\n",
    "        set_mlflow_tags(hparams, trial_number)\n",
    "\n",
    "        # Train, evaluate, and get the result\n",
    "        result = train_and_evaluate(hparams, trial_number)\n",
    "\n",
    "        # Save updated trials.pkl after each trial\n",
    "        save_trials(trials)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed8e1f75-a087-4f83-bd19-8a508ded0ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/16 23:20:09 INFO mlflow.tracking._tracking_service.client: 🏃 View run Hyperopt_Search at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/2/runs/09f24adde7ae4e589e0f1d3e05afc0fa.\n",
      "2024/09/16 23:20:09 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found: {'batch_size': 0, 'ent_coef': 0.008583590383175264, 'gae_lambda': 0.9421110523120086, 'gamma': 0.9207209601828463, 'n_epochs': 1, 'n_steps': 0}\n",
      "Best model saved at: models/best_model.zip\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "TOTAL_TIMESTEPS = 1000\n",
    "if chosen_device:\n",
    "    DEVICE = chosen_device\n",
    "else:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_EVALS = 20 \n",
    "ADDITIONAL_EVALS = 10\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "# Global variables\n",
    "models_dir = ensure_directory_exists(MODELS_DIR)\n",
    "best_model_path = os.path.join(models_dir, f\"best_model.zip\")\n",
    "best_mean_reward = -float(\"inf\")\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"reinforcement_learning/ppo/LunarLander\")\n",
    "\n",
    "# Load or initialize the Trials object\n",
    "if os.path.exists(\"trials.pkl\"):\n",
    "    with open(\"trials.pkl\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "        # Number of total trials you want to run (existing trials + new)\n",
    "    MAX_EVALS = len(trials) + ADDITIONAL_EVALS  # e.g., adding 10 more trials to the previous\n",
    "else:\n",
    "    trials = Trials()\n",
    "\n",
    "\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Save the updated Trials object for future use\n",
    "    with open(\"trials.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    mlflow.log_artifact(\"trials.pkl\")\n",
    "    \n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "    \n",
    "    # Log the best model as an artifact\n",
    "    mlflow.log_artifact(best_model_path)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)\n",
    "print(\"Best model saved at:\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c418d-4a00-4d95-bf5e-55dee1f55ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Specify the run ID for the best model (retrieve this from the MLflow UI or API)\n",
    "run_id = \"<run_id_for_best_model>\"\n",
    "local_dir = \"downloaded_model\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "\n",
    "# Download the best model artifact\n",
    "mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"model_trial_5.zip\", dst_path=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02ec05-6e5a-4553-8e67-e804bb74a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get the best run based on the `mean_reward` metric\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"0\",\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.mean_reward DESC\"],\n",
    "    max_results=1,\n",
    ")\n",
    "best_run = runs[0]\n",
    "best_hparams = best_run.data.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548378d-37e3-4432-9954-cb091b6f31f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb954b39-ec46-4dab-9f57-61b309dbda5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set metadata for the trial\n",
    "        mlflow.set_tag(\"trial_number\", trial_number)\n",
    "        mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "\n",
    "        # Log the hyperparameters for this trial\n",
    "        mlflow.log_params(hparams)\n",
    "\n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize the PPO model with the current hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # You can change this depending on the policy type you want\n",
    "            env=env,\n",
    "            **hparams\n",
    "        )\n",
    "\n",
    "        # Custom logger to log metrics to MLflow\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=10000, log_interval=1)\n",
    "\n",
    "        # Evaluation to calculate the score (mean reward)\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "        # Log the evaluation result as a metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Close environments\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "\n",
    "        # Return loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Helper function to evaluate the policy\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward, all_episode_rewards\n",
    "\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparameter_Optimization\")\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()  # Keep track of all trials\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f28693-29a9-417f-b0d4-70b2272435ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.9     |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 2567     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/16 19:13:47 INFO mlflow.tracking._tracking_service.client: 🏃 View run blushing-wasp-717 at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/0/runs/411a68d0dfce4e4885fe5ecdc42eb648.\n",
      "2024/09/16 19:13:47 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "hparams = {\n",
    "    'n_steps': 1024,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 4,\n",
    "    'gamma': 0.999,\n",
    "    'gae_lambda': 0.98,\n",
    "    'ent_coef': 0.01,\n",
    "}\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=\"logs\",\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log the hyperparameters to MLflow\n",
    "    mlflow.log_params(hparams)\n",
    "    \n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=2,\n",
    "        **hparams\n",
    "    )\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)\n",
    "    \n",
    "    env.reset()\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc94ec-bcc7-4caa-8abe-3b8312bd592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create environment\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "# Instantiate the agent\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e9b0-52fb-4059-a76d-5fe95dd67838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934a15c1-16ca-4c1d-add2-ea5c84bb5cb1",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f76eaa82-12f5-4d19-93a5-5c9a7d215d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving/ Resuming Trials\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(hparams)\n",
    "        \n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize PPO model with hyperparameters\n",
    "        model = PPO(policy=\"MlpPolicy\", env=env, **hparams)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        # Evaluate the model\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward = evaluate_policy(model, eval_env)\n",
    "\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "        return {'loss': -mean_reward, 'status': 'ok'}\n",
    "\n",
    "# Helper function to evaluate the model\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    # Evaluate the model and return the average reward\n",
    "    total_reward = 0\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "    mean_reward = total_reward / n_eval_episodes\n",
    "    return mean_reward\n",
    "\n",
    "# Load or initialize the Trials object\n",
    "if os.path.exists(\"trials.pkl\"):\n",
    "    with open(\"trials.pkl\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "else:\n",
    "    trials = Trials()\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparam_Optimization\")\n",
    "\n",
    "# Number of total trials you want to run (existing trials + new)\n",
    "new_max_evals = len(trials) + 10  # e.g., adding 10 more trials to the previous\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "best_hparams = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=new_max_evals,  # Total number of trials\n",
    "    trials=trials  # Resume from previous trials\n",
    ")\n",
    "\n",
    "# Save the updated Trials object for future use\n",
    "with open(\"trials.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trials, f)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3de590-fb43-44c6-9acf-176f25d25256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f488c5f-2df5-4dee-b92a-74adb89b6b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7356e97-167f-4649-be46-a293bb3a0b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a0ee4-6d6d-4e76-80d9-a5f8dc56bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b501a-3b82-4a1d-907b-5073032f02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84748a18-5f36-422e-833b-ad23b7583826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=None,\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=2)\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
