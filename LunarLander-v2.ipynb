{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f389030f-a5c8-4df8-8356-bdb9b23e9969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66b9cb-0ec3-4f1a-a3ff-a9d2f5cddf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q wheel setuptools pip swig --upgrade\n",
    "!pip install -q blinker --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d475f14-842e-49e4-826c-fe9bbe17bdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbf7883-2e9a-4753-bbd0-c51c184fa715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c4aa2-4f45-4e56-b279-0d6138f0cfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q stable-baselines3 \"gymnasium[box2d]\" tensorflow[and-cuda] dagshub mlflow hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794c57e-6164-4e64-b214-0800d3396f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb0e3f-6399-40d3-844e-47c48a377922",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "print(\"TensorRT version:\", trt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc19beb-5e67-4482-b3bc-5b1ad095e539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a461e-d0f3-49b6-ad14-c5aaf4588720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='smileynet', repo_name='gymnasium_experiments', mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe64222-285d-4d08-98c4-a74e0057f3c7",
   "metadata": {},
   "source": [
    "## Determine Best Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc7dc5-24c3-4884-9853-a768c033361e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "TOTAL_TIMESTEPS = 100000  # Short benchmark to measure FPS\n",
    "\n",
    "def measure_fps(device):\n",
    "    \"\"\"\n",
    "    Measure FPS on the given device (cpu or cuda).\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    # Initialize the model on the specified device\n",
    "    model = PPO('MlpPolicy', env, device=device)\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train for a small number of timesteps (benchmark)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    elapsed_time = end_time - start_time\n",
    "    fps = TOTAL_TIMESTEPS / elapsed_time\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n",
    "\n",
    "    return fps\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Measure FPS on CPU\n",
    "cpu_fps = measure_fps(device=\"cpu\")\n",
    "print(f\"CPU FPS: {cpu_fps:.2f}\")\n",
    "\n",
    "# Measure FPS on GPU (if available)\n",
    "if gpu_available:\n",
    "    gpu_fps = measure_fps(device=\"cuda\")\n",
    "    print(f\"GPU FPS: {gpu_fps:.2f}\")\n",
    "else:\n",
    "    gpu_fps = 0\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "# Select the device with the higher FPS\n",
    "if gpu_fps > cpu_fps:\n",
    "    print(\"Using GPU for training.\")\n",
    "    chosen_device = \"cuda\"\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "    chosen_device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee7f372-0f38-454e-af20-9cd501e5759f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "def ensure_directory_exists(new_dir):\n",
    "    if not os.path.exists(new_dir):\n",
    "        os.makedirs(new_dir)\n",
    "    return new_dir\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "                    \n",
    "\n",
    "def set_mlflow_tags(hparams, trial_number):\n",
    "    mlflow.set_tag(\"trial_number\", trial_number)\n",
    "    mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "    mlflow.set_tag(\"model_type\", \"PPO\")\n",
    "    mlflow.set_tag(\"policy_type\", \"MlpPolicy\")\n",
    "    mlflow.set_tag(\"environment_name\", \"LunarLander-v2\")\n",
    "    mlflow.set_tag(\"total_timesteps\", TOTAL_TIMESTEPS)\n",
    "    mlflow.set_tag(\"python_version\", sys.version)\n",
    "    mlflow.set_tag(\"stable_baselines3_version\", stable_baselines3.__version__)\n",
    "    mlflow.set_tag(\"device\", DEVICE)\n",
    "    mlflow.log_params(hparams)\n",
    "    \n",
    "def create_model(hparams, env):\n",
    "    return PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        device=DEVICE,\n",
    "        verbose=2,\n",
    "        **hparams\n",
    "    )\n",
    "\n",
    "def evaluate_model(model, eval_env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = eval_env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = eval_env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward\n",
    "\n",
    "def train_and_evaluate(hparams, trial_number):\n",
    "    global best_mean_reward\n",
    "    global models_dir\n",
    "    global best_model_path\n",
    "    \n",
    "    # Set up environments\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "    eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    try:\n",
    "        # Initialize the model\n",
    "        model = create_model(hparams, env)\n",
    "\n",
    "        # Set up logging\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[MLflowOutputFormat()], # HumanOutputFormat(sys.stdout), \n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=1)\n",
    "\n",
    "        # Evaluate the model\n",
    "        mean_reward = evaluate_model(model, eval_env)\n",
    "\n",
    "        # If best model, save it\n",
    "        if mean_reward > best_mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "            model_save_path = os.path.join(models_dir, f\"model_trial_{trial_number}.zip\")\n",
    "            model.save(model_save_path)\n",
    "            model.save(best_model_path)\n",
    "            mlflow.log_artifact(model_save_path)\n",
    "        \n",
    "        # Log the evaluation metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Return the loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "    finally:\n",
    "        # Ensure environments are properly closed\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "        \n",
    "def save_trials(trials):\n",
    "    with open(\"trials.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)        \n",
    "\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set MLflow tags\n",
    "        set_mlflow_tags(hparams, trial_number)\n",
    "\n",
    "        # Train, evaluate, and get the result\n",
    "        result = train_and_evaluate(hparams, trial_number)\n",
    "\n",
    "        # Save updated trials.pkl after each trial\n",
    "        save_trials(trials)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e1f75-a087-4f83-bd19-8a508ded0ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "TOTAL_TIMESTEPS = 1000000\n",
    "if 'chosen_device' in globals():\n",
    "    DEVICE = chosen_device\n",
    "else:\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_EVALS = 20 \n",
    "ADDITIONAL_EVALS = 10\n",
    "MODELS_DIR = \"models\"\n",
    "\n",
    "# Global variables\n",
    "models_dir = ensure_directory_exists(MODELS_DIR)\n",
    "best_model_path = os.path.join(models_dir, f\"best_model.zip\")\n",
    "best_mean_reward = -float(\"inf\")\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"reinforcement_learning/ppo/LunarLander\")\n",
    "\n",
    "# Load or initialize the Trials object\n",
    "if os.path.exists(\"trials.pkl\"):\n",
    "    with open(\"trials.pkl\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "        # Number of total trials you want to run (existing trials + new)\n",
    "    MAX_EVALS = len(trials) + ADDITIONAL_EVALS  # e.g., adding 10 more trials to the previous\n",
    "else:\n",
    "    trials = Trials()\n",
    "\n",
    "\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Save the updated Trials object for future use\n",
    "    with open(\"trials.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "    mlflow.log_artifact(\"trials.pkl\")\n",
    "    \n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "    \n",
    "    # Log the best model as an artifact\n",
    "    mlflow.log_artifact(best_model_path)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)\n",
    "print(\"Best model saved at:\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c418d-4a00-4d95-bf5e-55dee1f55ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Specify the run ID for the best model (retrieve this from the MLflow UI or API)\n",
    "run_id = \"<run_id_for_best_model>\"\n",
    "local_dir = \"downloaded_model\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "\n",
    "# Download the best model artifact\n",
    "mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"model_trial_5.zip\", dst_path=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02ec05-6e5a-4553-8e67-e804bb74a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get the best run based on the `mean_reward` metric\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"0\",\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.mean_reward DESC\"],\n",
    "    max_results=1,\n",
    ")\n",
    "best_run = runs[0]\n",
    "best_hparams = best_run.data.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548378d-37e3-4432-9954-cb091b6f31f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb954b39-ec46-4dab-9f57-61b309dbda5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set metadata for the trial\n",
    "        mlflow.set_tag(\"trial_number\", trial_number)\n",
    "        mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "\n",
    "        # Log the hyperparameters for this trial\n",
    "        mlflow.log_params(hparams)\n",
    "\n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize the PPO model with the current hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # You can change this depending on the policy type you want\n",
    "            env=env,\n",
    "            **hparams\n",
    "        )\n",
    "\n",
    "        # Custom logger to log metrics to MLflow\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=10000, log_interval=1)\n",
    "\n",
    "        # Evaluation to calculate the score (mean reward)\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "        # Log the evaluation result as a metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Close environments\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "\n",
    "        # Return loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Helper function to evaluate the policy\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward, all_episode_rewards\n",
    "\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparameter_Optimization\")\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()  # Keep track of all trials\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f28693-29a9-417f-b0d4-70b2272435ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "hparams = {\n",
    "    'n_steps': 1024,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 4,\n",
    "    'gamma': 0.999,\n",
    "    'gae_lambda': 0.98,\n",
    "    'ent_coef': 0.01,\n",
    "}\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=\"logs\",\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log the hyperparameters to MLflow\n",
    "    mlflow.log_params(hparams)\n",
    "    \n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=2,\n",
    "        **hparams\n",
    "    )\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)\n",
    "    \n",
    "    env.reset()\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc94ec-bcc7-4caa-8abe-3b8312bd592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create environment\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "# Instantiate the agent\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e9b0-52fb-4059-a76d-5fe95dd67838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934a15c1-16ca-4c1d-add2-ea5c84bb5cb1",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76eaa82-12f5-4d19-93a5-5c9a7d215d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving/ Resuming Trials\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(hparams)\n",
    "        \n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize PPO model with hyperparameters\n",
    "        model = PPO(policy=\"MlpPolicy\", env=env, **hparams)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        # Evaluate the model\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward = evaluate_policy(model, eval_env)\n",
    "\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "        return {'loss': -mean_reward, 'status': 'ok'}\n",
    "\n",
    "# Helper function to evaluate the model\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    # Evaluate the model and return the average reward\n",
    "    total_reward = 0\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "    mean_reward = total_reward / n_eval_episodes\n",
    "    return mean_reward\n",
    "\n",
    "# Load or initialize the Trials object\n",
    "if os.path.exists(\"trials.pkl\"):\n",
    "    with open(\"trials.pkl\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "else:\n",
    "    trials = Trials()\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparam_Optimization\")\n",
    "\n",
    "# Number of total trials you want to run (existing trials + new)\n",
    "new_max_evals = len(trials) + 10  # e.g., adding 10 more trials to the previous\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "best_hparams = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=new_max_evals,  # Total number of trials\n",
    "    trials=trials  # Resume from previous trials\n",
    ")\n",
    "\n",
    "# Save the updated Trials object for future use\n",
    "with open(\"trials.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trials, f)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3de590-fb43-44c6-9acf-176f25d25256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f488c5f-2df5-4dee-b92a-74adb89b6b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7356e97-167f-4649-be46-a293bb3a0b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a0ee4-6d6d-4e76-80d9-a5f8dc56bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b501a-3b82-4a1d-907b-5073032f02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84748a18-5f36-422e-833b-ad23b7583826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=None,\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=2)\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
