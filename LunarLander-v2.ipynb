{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f389030f-a5c8-4df8-8356-bdb9b23e9969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66b9cb-0ec3-4f1a-a3ff-a9d2f5cddf0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q wheel setuptools pip swig --upgrade\n",
    "!pip install -q blinker --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8c4aa2-4f45-4e56-b279-0d6138f0cfa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q stable-baselines3 \"gymnasium[box2d]\" tensorflow[and-cuda] dagshub mlflow hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794c57e-6164-4e64-b214-0800d3396f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc19beb-5e67-4482-b3bc-5b1ad095e539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5a461e-d0f3-49b6-ad14-c5aaf4588720",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as smileynet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as smileynet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"smileynet/gymnasium_experiments\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"smileynet/gymnasium_experiments\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository smileynet/gymnasium_experiments initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository smileynet/gymnasium_experiments initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='smileynet', repo_name='gymnasium_experiments', mlflow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe64222-285d-4d08-98c4-a74e0057f3c7",
   "metadata": {},
   "source": [
    "## Determine Best Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6bc7dc5-24c3-4884-9853-a768c033361e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU FPS: 167.55\n",
      "GPU FPS: 258.99\n",
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "TOTAL_TIMESTEPS = 100000  # Short benchmark to measure FPS\n",
    "\n",
    "def measure_fps(device):\n",
    "    \"\"\"\n",
    "    Measure FPS on the given device (cpu or cuda).\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    # Initialize the model on the specified device\n",
    "    model = PPO('MlpPolicy', env, device=device)\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train for a small number of timesteps (benchmark)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    elapsed_time = end_time - start_time\n",
    "    fps = TOTAL_TIMESTEPS / elapsed_time\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n",
    "\n",
    "    return fps\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Measure FPS on CPU\n",
    "cpu_fps = measure_fps(device=\"cpu\")\n",
    "print(f\"CPU FPS: {cpu_fps:.2f}\")\n",
    "\n",
    "# Measure FPS on GPU (if available)\n",
    "if gpu_available:\n",
    "    gpu_fps = measure_fps(device=\"cuda\")\n",
    "    print(f\"GPU FPS: {gpu_fps:.2f}\")\n",
    "else:\n",
    "    gpu_fps = 0\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "# Select the device with the higher FPS\n",
    "if gpu_fps > cpu_fps:\n",
    "    print(\"Using GPU for training.\")\n",
    "    chosen_device = \"cuda\"\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "    chosen_device = \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e1f75-a087-4f83-bd19-8a508ded0ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------                     \n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.9     |\n",
      "|    ep_rew_mean     | -166     |\n",
      "| time/              |          |\n",
      "|    fps             | 2612     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 106         |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1144        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011012673 |\n",
      "|    clip_fraction        | 0.0897      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0181     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 167         |\n",
      "|    n_updates            | 4           |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    value_loss           | 340         |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 110         |\n",
      "|    ep_rew_mean          | -127        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 961         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013312468 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 71.6        |\n",
      "|    n_updates            | 8           |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 188         |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | -86.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 897         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012865703 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 351         |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 140         |\n",
      "|    ep_rew_mean          | -89.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 850         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014431652 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 165         |\n",
      "|    ep_rew_mean          | -75.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 121         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012823088 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 51.8        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 173         |\n",
      "|    ep_rew_mean          | -90.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015383197 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 65.7        |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 99.5        |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 243         |\n",
      "|    ep_rew_mean          | -82.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 717         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 182         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010600077 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.6        |\n",
      "|    n_updates            | 28          |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 91          |\n",
      "-----------------------------------------\n",
      "\n",
      "-----------------------------------------             \n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 280         |\n",
      "|    ep_rew_mean          | -63.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 685         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 215         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014249215 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00967    |\n",
      "|    value_loss           | 62          |\n",
      "-----------------------------------------\n",
      "\n",
      "  0%|          | 0/20 [03:36<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "import torch\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "TOTAL_TIMESTEPS = 1000000\n",
    "DEVICE = chosen_device\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set metadata for the trial\n",
    "        mlflow.set_tag(\"trial_number\", trial_number)\n",
    "        mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "        mlflow.set_tag(\"model_type\", \"PPO\")\n",
    "        mlflow.set_tag(\"policy_type\", \"MlpPolicy\")\n",
    "        mlflow.set_tag(\"environment_name\", \"LunarLander-v2\")\n",
    "        mlflow.set_tag(\"total_timesteps\", TOTAL_TIMESTEPS)\n",
    "        mlflow.set_tag(\"python_version\", sys.version)\n",
    "        mlflow.set_tag(\"stable_baselines3_version\", stable_baselines3.__version__)\n",
    "\n",
    "        # Check if GPU is available\n",
    "        #device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        device=DEVICE\n",
    "        mlflow.set_tag(\"device\", device)\n",
    "        \n",
    "        # Log the hyperparameters for this trial\n",
    "        mlflow.log_params(hparams)\n",
    "\n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize the PPO model with the current hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # You can change this depending on the policy type you want\n",
    "            env=env,\n",
    "            device=device,\n",
    "            verbose=2\n",
    "            **hparams\n",
    "        )\n",
    "\n",
    "        # Custom logger to log metrics to MLflow\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=TOTAL_TIMESTEPS, log_interval=1)\n",
    "\n",
    "        # Evaluation to calculate the score (mean reward)\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "        # Log the evaluation result as a metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Save the model if it's the best so far\n",
    "        if mean_reward > best_mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "\n",
    "            # Save model to a file\n",
    "            model_save_path = f\"model_trial_{trial_number}.zip\"\n",
    "            model.save(model_save_path)\n",
    "            best_model_path = model_save_path\n",
    "\n",
    "            # Log the best model artifact in MLflow\n",
    "            mlflow.log_artifact(best_model_path)\n",
    "        \n",
    "        # Close environments\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "\n",
    "        # Return loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Helper function to evaluate the policy\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward, all_episode_rewards\n",
    "\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"reinforcement_learning/ppo/LunarLander\")\n",
    "\n",
    "# Global variable to track the best model and reward\n",
    "best_mean_reward = -float(\"inf\")\n",
    "best_model_path = \"best_model.zip\"\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()  # Keep track of all trials\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "    \n",
    "    # Log the best model as an artifact\n",
    "    mlflow.log_artifact(best_model_path)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)\n",
    "print(\"Best model saved at:\", best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8c418d-4a00-4d95-bf5e-55dee1f55ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Specify the run ID for the best model (retrieve this from the MLflow UI or API)\n",
    "run_id = \"<run_id_for_best_model>\"\n",
    "local_dir = \"downloaded_model\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.makedirs(local_dir)\n",
    "\n",
    "# Download the best model artifact\n",
    "mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=\"model_trial_5.zip\", dst_path=local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02ec05-6e5a-4553-8e67-e804bb74a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Get the best run based on the `mean_reward` metric\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=\"0\",\n",
    "    filter_string=\"\",\n",
    "    order_by=[\"metrics.mean_reward DESC\"],\n",
    "    max_results=1,\n",
    ")\n",
    "best_run = runs[0]\n",
    "best_hparams = best_run.data.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9548378d-37e3-4432-9954-cb091b6f31f4",
   "metadata": {},
   "source": [
    "## Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb954b39-ec46-4dab-9f57-61b309dbda5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "# Custom logging format to send metrics to MLflow\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space for Hyperopt\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams, trial_number):\n",
    "    with mlflow.start_run(nested=True):  # Nested run for each trial\n",
    "        # Set metadata for the trial\n",
    "        mlflow.set_tag(\"trial_number\", trial_number)\n",
    "        mlflow.set_tag(\"optimizer\", \"hyperopt\")\n",
    "\n",
    "        # Log the hyperparameters for this trial\n",
    "        mlflow.log_params(hparams)\n",
    "\n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize the PPO model with the current hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # You can change this depending on the policy type you want\n",
    "            env=env,\n",
    "            **hparams\n",
    "        )\n",
    "\n",
    "        # Custom logger to log metrics to MLflow\n",
    "        loggers = Logger(\n",
    "            folder=\"logs\",\n",
    "            output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    "        )\n",
    "        model.set_logger(loggers)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=10000, log_interval=1)\n",
    "\n",
    "        # Evaluation to calculate the score (mean reward)\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "\n",
    "        # Log the evaluation result as a metric\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "\n",
    "        # Close environments\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "\n",
    "        # Return loss (negative reward to minimize)\n",
    "        return {'loss': -mean_reward, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Helper function to evaluate the policy\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        episode_rewards = 0\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards += reward\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "    mean_reward = np.mean(all_episode_rewards)\n",
    "    return mean_reward, all_episode_rewards\n",
    "\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparameter_Optimization\")\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "trials = Trials()  # Keep track of all trials\n",
    "\n",
    "# Start a single MLflow run to track the entire optimization process\n",
    "with mlflow.start_run(run_name=\"Hyperopt_Search\"):\n",
    "    best_hparams = fmin(\n",
    "        fn=lambda hparams: objective(hparams, len(trials)),  # Pass trial number\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=20,  # Number of evaluations\n",
    "        trials=trials\n",
    "    )\n",
    "\n",
    "    # Log the best hyperparameters found after optimization\n",
    "    mlflow.log_params({\"best_hparams\": best_hparams})\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f28693-29a9-417f-b0d4-70b2272435ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.9     |\n",
      "|    ep_rew_mean     | -187     |\n",
      "| time/              |          |\n",
      "|    fps             | 2567     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/16 19:13:47 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run blushing-wasp-717 at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/0/runs/411a68d0dfce4e4885fe5ecdc42eb648.\n",
      "2024/09/16 19:13:47 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: https://dagshub.com/smileynet/gymnasium_experiments.mlflow/#/experiments/0.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "hparams = {\n",
    "    'n_steps': 1024,\n",
    "    'batch_size': 64,\n",
    "    'n_epochs': 4,\n",
    "    'gamma': 0.999,\n",
    "    'gae_lambda': 0.98,\n",
    "    'ent_coef': 0.01,\n",
    "}\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=\"logs\",\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log the hyperparameters to MLflow\n",
    "    mlflow.log_params(hparams)\n",
    "    \n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "    \n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        verbose=2,\n",
    "        **hparams\n",
    "    )\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)\n",
    "    \n",
    "    env.reset()\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc94ec-bcc7-4caa-8abe-3b8312bd592a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create environment\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "# Instantiate the agent\n",
    "# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\n",
    "# if we had frames as input we would use CnnPolicy\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=64,\n",
    "    n_epochs=4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.98,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train it for 1,000,000 timesteps\n",
    "model.learn(total_timesteps=1000000)\n",
    "\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08e9b0-52fb-4059-a76d-5fe95dd67838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934a15c1-16ca-4c1d-add2-ea5c84bb5cb1",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f76eaa82-12f5-4d19-93a5-5c9a7d215d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving/ Resuming Trials\n",
    "import os\n",
    "import pickle\n",
    "import mlflow\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    'n_steps': hp.choice('n_steps', [512, 1024, 2048]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'n_epochs': hp.choice('n_epochs', [3, 4, 5]),\n",
    "    'gamma': hp.uniform('gamma', 0.9, 0.999),\n",
    "    'gae_lambda': hp.uniform('gae_lambda', 0.8, 1.0),\n",
    "    'ent_coef': hp.uniform('ent_coef', 0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Objective function for Hyperopt\n",
    "def objective(hparams):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        mlflow.log_params(hparams)\n",
    "        \n",
    "        # Create the environment\n",
    "        env = make_vec_env(\"LunarLander-v2\", n_envs=16)\n",
    "\n",
    "        # Initialize PPO model with hyperparameters\n",
    "        model = PPO(policy=\"MlpPolicy\", env=env, **hparams)\n",
    "        model.learn(total_timesteps=10000)\n",
    "\n",
    "        # Evaluate the model\n",
    "        eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "        mean_reward = evaluate_policy(model, eval_env)\n",
    "\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "        return {'loss': -mean_reward, 'status': 'ok'}\n",
    "\n",
    "# Helper function to evaluate the model\n",
    "def evaluate_policy(model, env, n_eval_episodes=10):\n",
    "    # Evaluate the model and return the average reward\n",
    "    total_reward = 0\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "    mean_reward = total_reward / n_eval_episodes\n",
    "    return mean_reward\n",
    "\n",
    "# Load or initialize the Trials object\n",
    "if os.path.exists(\"trials.pkl\"):\n",
    "    with open(\"trials.pkl\", \"rb\") as f:\n",
    "        trials = pickle.load(f)\n",
    "else:\n",
    "    trials = Trials()\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"LunarLander_Hyperparam_Optimization\")\n",
    "\n",
    "# Number of total trials you want to run (existing trials + new)\n",
    "new_max_evals = len(trials) + 10  # e.g., adding 10 more trials to the previous\n",
    "\n",
    "# Run Hyperopt optimization\n",
    "best_hparams = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=new_max_evals,  # Total number of trials\n",
    "    trials=trials  # Resume from previous trials\n",
    ")\n",
    "\n",
    "# Save the updated Trials object for future use\n",
    "with open(\"trials.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trials, f)\n",
    "\n",
    "print(\"Best hyperparameters found:\", best_hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3de590-fb43-44c6-9acf-176f25d25256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f488c5f-2df5-4dee-b92a-74adb89b6b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7356e97-167f-4649-be46-a293bb3a0b26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10_000)\n",
    "\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a0ee4-6d6d-4e76-80d9-a5f8dc56bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent\n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b501a-3b82-4a1d-907b-5073032f02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine ðŸ˜„\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84748a18-5f36-422e-833b-ad23b7583826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, Tuple, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger\n",
    "\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    \"\"\"\n",
    "    Dumps key/value pairs into MLflow's numeric format.\n",
    "    \"\"\"\n",
    "\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "\n",
    "            if isinstance(value, np.ScalarType):\n",
    "                if not isinstance(value, str):\n",
    "                    mlflow.log_metric(key, value, step)\n",
    "\n",
    "\n",
    "loggers = Logger(\n",
    "    folder=None,\n",
    "    output_formats=[HumanOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    ")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    model = SAC(\"MlpPolicy\", \"Pendulum-v1\", verbose=2)\n",
    "    # Set custom logger\n",
    "    model.set_logger(loggers)\n",
    "    model.learn(total_timesteps=10000, log_interval=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
