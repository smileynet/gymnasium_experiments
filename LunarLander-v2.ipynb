{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f389030f-a5c8-4df8-8356-bdb9b23e9969",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b577264-b033-49be-997d-eeb77097241b",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install --upgrade pip wheel setuptools swig",
   "id": "e71fd8a50f194103",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install ipywidgets",
   "id": "9749657f46fa5bc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install --upgrade tensorrt\n",
    "#%pip install nvidia-tensorrt --extra-index-url https://pypi.ngc.nvidia.com\n"
   ],
   "id": "1ce51513bea880c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "473031d28f4c518e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install --upgrade tensorflow[and-cuda]\n",
    "#%pip install tensorflow-metal "
   ],
   "id": "ca5a8ce04d218129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install blinker --ignore-installed\n",
   "id": "a68afb1f7a5a9ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "source": "%pip install mlflow optuna optuna-integration[mlflow] plotly\n",
   "id": "0f66b9cb-0ec3-4f1a-a3ff-a9d2f5cddf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install stable-baselines3 dagshub \n",
   "id": "f1238ec5667da9d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install gymnasium",
   "id": "b79aebd5b9bdff97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install gymnasium[box2d]",
   "id": "e651ba87a0be9371",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install \"stable-baselines3[extra]\" PyMySQL python-dotenv\n",
   "id": "53653aa39e7a5f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "source": [
    "import tensorrt as trt\n",
    "print(\"TensorRT version:\", trt.__version__)\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "81bb0e3f-6399-40d3-844e-47c48a377922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ce4b5641feb7392e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "df5a461e-d0f3-49b6-ad14-c5aaf4588720",
   "metadata": {
    "tags": []
   },
   "source": [
    "import dagshub\n",
    "dagshub.init(repo_owner='smileynet', repo_name='gymnasium_experiments', mlflow=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ],
   "id": "29ff1d611140eceb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip freeze > requirements.txt",
   "id": "791666ec2f0e51e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Fetch database connection details from environment variables\n",
    "db_name = os.getenv(\"DB_NAME\")\n",
    "db_user = os.getenv(\"DB_USER\")\n",
    "db_password = os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv(\"DB_HOST\")\n",
    "db_port = os.getenv(\"DB_PORT\")\n",
    "\n",
    "# Construct the PostgreSQL connection URL\n",
    "mysql_url = f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "print(\"Database connection URL: \", mysql_url)  # For testing purposes, you can print this but avoid showing sensitive data in production."
   ],
   "id": "fa922be2-78b8-48ab-8a1c-ed9758162bbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "faa2e5f9-8292-45dc-977b-2756d61e7b23",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f4d7b73-f9c8-4864-929e-7de722f876a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "import gymnasium as gym\n",
    "import optuna\n",
    "import os\n",
    "import mlflow\n",
    "import torch\n",
    "import sys\n",
    "import gc\n",
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from typing import Any, Dict, Union, Tuple\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.logger import HumanOutputFormat, KVWriter, Logger, configure\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Define constants\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_TRIALS = 15\n",
    "N_ENVS = 64\n",
    "TOTAL_STEPS = 1000000\n",
    "DATA_DIR = \"data\"\n",
    "MODELS_DIR = \"models\"\n",
    "LOGS_DIR = \"logs\"\n",
    "\n",
    "# Ensure the models directory exists\n",
    "def ensure_directory_exists(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    return directory\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = ensure_directory_exists(MODELS_DIR)\n",
    "data_dir = ensure_directory_exists(DATA_DIR)\n",
    "logs_dir = ensure_directory_exists(LOGS_DIR)\n",
    "\n",
    "best_model_path = os.path.join(models_dir, \"best_model.zip\")\n",
    "best_mean_reward = -float(\"inf\")\n",
    "db_path = os.path.join(data_dir, 'study.db')\n",
    "local_storage = f\"sqlite:///{db_path}\"\n",
    "\n",
    "\n",
    "rdb_storage = optuna.storages.RDBStorage(\n",
    "    url = mysql_url,\n",
    "    #heartbeat_interval=60, \n",
    "    #grace_period=120\n",
    "    )\n",
    "\n",
    "storage = rdb_storage\n",
    "\n",
    "# Set up MLflow experiment tracking\n",
    "mlflow.set_experiment(\"PPO-LunarLander-v2\")\n",
    "\n",
    "class ClearOutputFormat(HumanOutputFormat):\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]] = {},\n",
    "        step: int = 0\n",
    "    ) -> None:\n",
    "        clear_output(wait=True)\n",
    "        super().write(key_values, key_excluded, step)\n",
    "\n",
    "class MLflowOutputFormat(KVWriter):\n",
    "    def write(\n",
    "        self,\n",
    "        key_values: Dict[str, Any],\n",
    "        key_excluded: Dict[str, Union[str, Tuple[str, ...]]],\n",
    "        step: int = 0,\n",
    "    ) -> None:\n",
    "        for (key, value), (_, excluded) in zip(\n",
    "            sorted(key_values.items()), sorted(key_excluded.items())\n",
    "        ):\n",
    "            if excluded is not None and \"mlflow\" in excluded:\n",
    "                continue\n",
    "            # Explicitly check for valid types before logging\n",
    "            if isinstance(value, (int, float, np.integer, np.floating)) and not isinstance(value, str):\n",
    "                mlflow.log_metric(key, value, step)\n",
    "            else:\n",
    "                print(f\"invalid metric of {key}: {value}\")\n",
    "                    \n",
    "                    \n",
    "def filter_valid_params(hparams):\n",
    "    \"\"\"Filter valid MLflow parameter types.\"\"\"\n",
    "    valid_params = {}\n",
    "    for k, v in hparams.items():\n",
    "        if isinstance(v, (int, float, str)):  # Check for valid types\n",
    "            valid_params[k] = v\n",
    "        else:\n",
    "            print(f\"Skipping invalid parameter: {k} = {v} (type: {type(v)})\")\n",
    "    return valid_params\n",
    "\n",
    "# Helper function to evaluate the model\n",
    "def evaluate_model(model, env, num_episodes=10):\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "    return sum(total_rewards) / len(total_rewards)\n",
    "\n",
    "\n",
    "# Function to set MLflow tags\n",
    "def set_mlflow_tags(hparams, trial_number):\n",
    "    tags = {\n",
    "        \"trial_number\": trial_number,\n",
    "        \"algorithm\": \"PPO\",\n",
    "        \"type\": \"reinforcement_learning\",\n",
    "        \"optimizer\": \"optuna\",\n",
    "        \"policy_type\": \"MlpPolicy\",\n",
    "        \"environment_name\": \"LunarLander-v2\",\n",
    "        \"total_timesteps\": TOTAL_STEPS,\n",
    "        \"n_envs\": N_ENVS,\n",
    "        \"python_version\": sys.version,\n",
    "        \"stable_baselines3_version\": PPO.__module__.split(\".\")[1],\n",
    "        \"device\": DEVICE\n",
    "    }\n",
    "    mlflow.set_tags(tags)\n",
    "    #filtered_hparams = filter_valid_params(hparams)\n",
    "    mlflow.log_params(hparams)\n",
    "\n",
    "# Function to create the PPO model\n",
    "def create_model(hparams, env, checkpoint_path=None):\n",
    "    # Create the PPO model\n",
    "    model = PPO(policy=\"MlpPolicy\", env=env, device=DEVICE, verbose=2, **hparams)\n",
    "\n",
    "    # Load from checkpoint if available\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        model.load(checkpoint_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Main function to train and evaluate the model\n",
    "def train_and_evaluate(hparams, trial_number, trial):\n",
    "    global best_mean_reward, best_model_path\n",
    "\n",
    "    # Setup environments\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=N_ENVS)\n",
    "    eval_env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    checkpoint_path = os.path.join(models_dir, \"checkpoint.zip\")\n",
    "\n",
    "    try:\n",
    "        model = create_model(hparams, env, checkpoint_path)\n",
    "\n",
    "        # Setup logger with the custom callbacks\n",
    "        logger = Logger(\n",
    "            folder=logs_dir,\n",
    "            output_formats=[ClearOutputFormat(sys.stdout), MLflowOutputFormat()],\n",
    "        )\n",
    "        model.set_logger(logger)\n",
    "\n",
    "        # Train the model\n",
    "        model.learn(total_timesteps=TOTAL_STEPS) \n",
    "\n",
    "        # Evaluate the model\n",
    "        mean_reward = evaluate_model(model, eval_env)\n",
    "        mlflow.log_metric(\"mean_reward\", mean_reward)\n",
    "        \n",
    "\n",
    "        # Save the model if it's the best one so far\n",
    "        if mean_reward > best_mean_reward:\n",
    "            best_mean_reward = mean_reward\n",
    "            model.save(best_model_path)\n",
    "            print(f\"New best model saved with mean reward: {mean_reward}\")\n",
    "\n",
    "        return mean_reward\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        print(\"Concluding trial...\")\n",
    "        env.reset()\n",
    "        env.close()\n",
    "        eval_env.reset()\n",
    "        eval_env.close()\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    hparams = {\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [512, 1024, 2048]),\n",
    "        'n_steps': trial.suggest_categorical('n_steps', [32, 64, 128]),\n",
    "        'n_epochs': trial.suggest_int('n_epochs', 3, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.9, 0.999),\n",
    "        'gae_lambda': trial.suggest_float('gae_lambda', 0.8, 1.0),\n",
    "        'ent_coef': trial.suggest_float('ent_coef', 0.0001, 0.01),\n",
    "    }\n",
    "\n",
    "    # Start an MLflow run to log this trial\n",
    "    with mlflow.start_run(nested=True):\n",
    "        set_mlflow_tags(hparams, trial.number)\n",
    "        result = train_and_evaluate(hparams, trial.number, trial)\n",
    "        print(\"Exiting MLFlow run...\")\n",
    "        return result\n",
    "\n",
    "# Main optimization loop\n",
    "def main():\n",
    "    # Define a pruner to stop unpromising trials early\n",
    "    #pruner = optuna.pruners.MedianPruner()\n",
    "    \n",
    "    # Create or load the study\n",
    "    study = optuna.create_study(\n",
    "        study_name=\"1M_steps\", \n",
    "        direction=\"maximize\", \n",
    "        #pruner=pruner,\n",
    "        storage=storage, \n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    # Run the optimization with parallel jobs\n",
    "    study.optimize(\n",
    "        objective,  \n",
    "        n_trials=MAX_TRIALS, \n",
    "        #n_jobs=1,\n",
    "        #gc_after_trial=True\n",
    "    )\n",
    "    print(\"Exiting study...\")\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"Best trial: {best_trial.number}\")\n",
    "    print(f\"Value: {best_trial.value}\")\n",
    "    print(\"Params: \")\n",
    "    for key, value in best_trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "        \n",
    "    # Save the best hyperparameters\n",
    "    best_hparams = study.best_params\n",
    "    print(f\"Best Hyperparameters: {best_hparams}\")\n",
    "\n",
    "    # Log the best model as an artifact in MLflow\n",
    "    mlflow.log_params(best_hparams)\n",
    "    mlflow.log_artifact(best_model_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8573c767-04d1-49e1-8ddd-b2eaf8053387",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the main function\n",
    "#if __name__ == \"__main__\":\n",
    "main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25025cde-aef0-44df-ab24-809c8ee2419b",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "study = optuna.study.load_study(study_name='1M_steps', storage=storage)"
   ],
   "id": "1850498675302224",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "13d8dfff23e0c7ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial.number}\")\n",
    "print(f\"Value: {best_trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ],
   "id": "2933bee2cc060e59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\"))\n",
    "print(df)"
   ],
   "id": "cd8e098a842cfbfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_rank\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_timeline\n",
    "\n",
    "plot_optimization_history(study)\n",
    "\n"
   ],
   "id": "26569d1a04f8351b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_intermediate_values(study)\n",
   "id": "58161eefd247734f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_parallel_coordinate(study)",
   "id": "af069aa8b26fc17a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_contour(study)\n",
   "id": "6854846f3c540281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_slice(study)\n",
   "id": "94ba4ffb561c95e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_param_importances(study)\n",
   "id": "1ecdda521c0d8e9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_edf(study)\n",
   "id": "c18536500614dfde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_rank(study)\n",
   "id": "27276e9cd7eb7eb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_timeline(study)",
   "id": "86fba2c7a2c292cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install -U kaleido",
   "id": "bef5b41445c5d705",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7479eeb2-a45d-4a0c-9f0a-697df42fbabd",
   "metadata": {},
   "source": [
    "import plotly\n",
    "\n",
    "# Visualize optimization history and parameter importances\n",
    "optimization_history_figure = plot_optimization_history(study)\n",
    "optimization_history_figure.write_image(\"optimization_history.png\")\n",
    "mlflow.log_artifact(\"optimization_history.png\")\n",
    "\n",
    "param_importance_figure = plot_param_importances(study)\n",
    "param_importance_figure.write_image(\"param_importances.png\")\n",
    "mlflow.log_artifact(\"param_importances.png\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f488c5f-2df5-4dee-b92a-74adb89b6b98",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cbe64222-285d-4d08-98c4-a74e0057f3c7",
   "metadata": {},
   "source": [
    "## Benchmark CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6bc7dc5-24c3-4884-9853-a768c033361e",
   "metadata": {
    "tags": []
   },
   "source": [
    "import time\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "TOTAL_TIMESTEPS = 100000  # Short benchmark to measure FPS\n",
    "\n",
    "def measure_fps(device):\n",
    "    \"\"\"\n",
    "    Measure FPS on the given device (cpu or cuda).\n",
    "    \"\"\"\n",
    "    # Create environment\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=1)\n",
    "\n",
    "    # Initialize the model on the specified device\n",
    "    model = PPO('MlpPolicy', env, device=device)\n",
    "\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train for a small number of timesteps (benchmark)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS)\n",
    "\n",
    "    # End timer\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    elapsed_time = end_time - start_time\n",
    "    fps = TOTAL_TIMESTEPS / elapsed_time\n",
    "\n",
    "    # Clean up\n",
    "    env.close()\n",
    "\n",
    "    return fps\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "# Measure FPS on CPU\n",
    "cpu_fps = measure_fps(device=\"cpu\")\n",
    "print(f\"CPU FPS: {cpu_fps:.2f}\")\n",
    "\n",
    "# Measure FPS on GPU (if available)\n",
    "if gpu_available:\n",
    "    gpu_fps = measure_fps(device=\"cuda\")\n",
    "    print(f\"GPU FPS: {gpu_fps:.2f}\")\n",
    "else:\n",
    "    gpu_fps = 0\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "# Select the device with the higher FPS\n",
    "if gpu_fps > cpu_fps:\n",
    "    print(\"Using GPU for training.\")\n",
    "    chosen_device = \"cuda\"\n",
    "else:\n",
    "    print(\"Using CPU for training.\")\n",
    "    chosen_device = \"cpu\"\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "934a15c1-16ca-4c1d-add2-ea5c84bb5cb1",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b1a0ee4-6d6d-4e76-80d9-a5f8dc56bb6b",
   "metadata": {},
   "source": [
    "# TODO: Evaluate the agent with this instead\n",
    "\n",
    "# Create a new environment for evaluation\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "\n",
    "# Evaluate the model with 10 evaluation episodes and deterministic=True\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "# Print the results\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f30b501a-3b82-4a1d-907b-5073032f02d1",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(\n",
    "    model=model,  # Our trained model\n",
    "    model_name=model_name,  # The name of our trained model\n",
    "    model_architecture=model_architecture,  # The model architecture we used: in our case PPO\n",
    "    env_id=env_id,  # Name of the environment\n",
    "    eval_env=eval_env,  # Evaluation Environment\n",
    "    repo_id=repo_id,  # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "    commit_message=commit_message,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
